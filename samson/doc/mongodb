INSTALLATION:
-------------------------------------------------------------------------------
  o Download the two tar files 'mongodb-linux-x86_64-2.0.0.tgz' and 'mongodb-linux-x86_64-latest.tgz'
    Found in 'http://www.mongodb.org/downloads'

  o Uncompress both in /opt/mongo
    % sudo mkdir /opt/mongodb
    % sudo chown samson:samson /opt/mongodb
    % mv ~/Downloads/mongodb-linux-x86_64-2.0.0.tgz /opt/mongodb
    % mv ~/Downloads/mongodb-linux-x86_64-latest.tgz /opt/mongodb
    % cd /opt/mongodb
    % tar xvf mongodb-linux-x86_64-2.0.0.tgz
    % tar xvf mongodb-linux-x86_64-latest.tgz
    % mv mongodb-linux-x86_64-latest mongo-cxx-driver-v2.0

  o Install 'boost':
    RED HAT
      % sudo yum install boost-devel.x86_64

    UBUNTU
      % sudo aptitude install libboost-dev
      % sudo aptitude install libboost-date-time1.40-dev
      % sudo aptitude install libboost-filesystem-dev
      % sudo aptitude install libboost-system-dev
      % sudo aptitude install libboost-thread-dev
      % sudo aptitude install libboost-program-options1.40.0 
      % sudo aptitude install libboost-regex1.40.0

  o Recompile 'libmongoclient.a' with '-fPIC'
    o Downloading scons from 'http://sourceforge.net/projects/scons/files/scons/2.0.1/'
    o Installing scons with:
      % rpm -i scons-2.0.1-1.noarch.rpm
    o Edit SConstruct, adding '-fPIC' to CPPFLAGS:
      - env.Append( CPPFLAGS=" -O3" )
      + env.Append( CPPFLAGS=" -O3 -fPIC" )
    o Install 'pcre':
      % sudo yum install pcre-devel.x86_64
    o Now compile:
      % scons libmongoclient.a

  o Compile the tutorial:
    % edit client/examples/tutorial.cpp
      CHANGE ../../client/dbclient.h   FOR ../../mongo/client/dbclient.h
    % Compile 'tutorial':
      g++ tutorial.cpp -L /opt/mongodb/mongo-cxx-driver-v2.0/ -lmongoclient -lboost_thread -lboost_filesystem -o tutorial
    

  o Now samson modules compile just fine - had more problems in 32bit mode ...
    There I had to resompile 4-5 files by hand, having a look in '/opt/mongodb/mongo-cxx-driver-v2.0/config.log'
===============================================================================


STARTING A CLUSTER (master in samson03, slave in samson04)
-------------------------------------------------------------------------------
samson03> sudo mkdir /data
samson03> sudo mkdir /data/masterdb
samson03> sudo chown samson:samson /data/masterdb

samson03> cd /opt/mongodb/mongodb-linux-x86_64-2.0.0/
samson03> bin/mongod --master --dbpath /data/masterdb/

samson04> sudo mkdir /data
samson04> sudo mkdir /data/slavedb
samson04> sudo chown samson:samson /data/slavedb

samson04> cd /opt/mongodb/mongodb-linux-x86_64-2.0.0/
samson04> bin/mongod --slave --source samson03 --dbpath /data/slavedb/
===============================================================================

STARTING THE MONGO SHELL
-------------------------------------------------------------------------------
% cd /opt/mongodb/mongodb-linux-x86_64-2.0.0/
% bin/mongo                         # connects to a 'test' database, that doesn't exist ...
% bin/mongo DBNAME                  # connects to the 'DBNAME' database
% bin/mongo tora/kz                 # connects to the 'kz' database on host 'tora'
% bin/mongo tora:9999/kz            # same thing but on port 9999
===============================================================================


Source: http://www.mongodb.org/display/DOCS/Manual

GETTING HELP (I)
-------------------------------------------------------------------------------
% mongo --help
MongoDB shell version: 2.0.0
usage: /opt/mongodb/mongodb-linux-x86_64-2.0.0/bin/mongo [options] [db address] [file names (ending in .js)]

db address can be:
  foo                   foo database on local machine
  192.169.0.5/foo       foo database on 192.168.0.5 machine
  192.169.0.5:9999/foo  foo database on 192.168.0.5 machine on port 9999

options:
  --shell               run the shell after executing files
  --nodb                don't connect to mongod on startup - no 'db address' 
                        arg expected
  --quiet               be less chatty
  --port arg            port to connect to
  --host arg            server to connect to
  --eval arg            evaluate javascript
  -u [ --username ] arg username for authentication
  -p [ --password ] arg password for authentication
  -h [ --help ]         show this usage information
  --version             show version information
  --verbose             increase verbosity
  --ipv6                enable IPv6 support (disabled by default)

file names: a list of files to run. files have to end in .js and will exit after unless --shell is specified
===============================================================================

GETTING HELP (II)
-------------------------------------------------------------------------------
% mongo
> help
        db.help()                    help on db methods
        db.mycoll.help()             help on collection methods
        rs.help()                    help on replica set methods
        help connect                 connecting to a db help
        help admin                   administrative help
        help misc                    misc things to know
        help mr                      mapreduce help

        show dbs                     show database names
        show collections             show collections in current database
        show users                   show users in current database
        show profile                 show most recent system.profile entries with time >= 1ms
        use <db_name>                set current database
        db.foo.find()                list objects in collection foo
        db.foo.find( { a : 1 } )     list objects in foo where a == 1
        it                           result of the last line evaluated; use to further iterate
        DBQuery.shellBatchSize = x   set default number of items to display on shell
        exit                         quit the mongo shell
===============================================================================

MONGO COMMAND LINE - INTRO
-------------------------------------------------------------------------------
Many emacs commands work:
  Start/End of line:   Ctrl-A / Ctrl-E
  History:             Arrows / Ctrl-P / Ctrl-N
  Search:              Ctrl-R
  Deletion/Yanking:    ESC-D, ESC-Del, Ctrl-W (delete word) Ctrl-K / Ctrl-Y, ESC-Y
  Params:              ESC-.

The mongo shell is a JavaScript interpreter

Try:
  > 5 + 5
  10

  > function fact(x) {
  ...  if (x == 1) return 1;
  ...  return x * fact(x - 1);
  ...  }
  > fact(5)
  720

  > fact
  function fact(x) {
      if (x == 1) {
          return 1;
      }
      return x * fact(x - 1);
  }

===============================================================================

MONGO COMMAND LINE - DATABASES
-------------------------------------------------------------------------------

Basics:
> show dbs
> use DB
> show collections


Create a database:
Just 'use' one and when something inserted, it is created:
> use kzdb


Create a collection (this one is capped ...):
> db.createCollection("people", {capped:true, size:100000})

> show collections

Insert data to the collection (difference between 'save' and 'insert' ... save does upsert (update/insert?))
 db.people.insert({ firstname: "Ken",    age: 45, lastname: "Zangelin" })
 db.people.insert({ firstname: "Malika", age: 28, lastname: "Fazulova" })
 db.people.insert({ firstname: "Rosana", age: 43, lastname: "Calvo" })
 db.people.insert({ firstname: "Alex",   age: 12, lastname: "Zangelin" })
 db.people.insert({ firstname: "Maria",  age:  7, lastname: "Zangelin" }) 


Query Lookup:
> DBQuery.shellBatchSize = 3
> db.people.find()
> it
> db.people.find().limit(2)

> db.people.find( { name : "Ken" } )
> db.people.findOne({"firstname" : "Maria"}) 
> db.people.count()
> db.people.find( { firstname : "Ken" } ).count()
> db.people.find( { $or : [ { name : "Ken" } , { age : 28 } ] } )


Lookup using 'Greater Than', etc:
> db.people.find( { "age" : { $gt:   7 } } )
> db.people.find( { "age" : { $gte:  7 } } )
> db.people.find( { "age" : { $lt:  12 } } )
> db.people.find( { "age" : { $lte: 12 } } )
> db.people.find( { "age" : { $ne:  45 } } )
> db.people.find( { "age" : { $in: [7,12] } } )
> db.people.find( { "age" : { $in: [7,12] } } ).count()

Selecting items 'SELECT * WHERE ...':
> db.people.find( { "age" : { $gt: 14, $lt: 45 } } );
> db.people.find( { "age" : { $gt: 14, $lt: 47 }, "firstname" : "Ken" } );

Only show specified FIELDS:
> db.people.find( { "age" : { $gt: 14, $lt: 47 }, "firstname" : "Ken" }, {firstname:1} );

Create an Index
> db.people.ensureIndex({age:1})  # age:-1 would create the index in the 'other direction' ...
> db.people.getIndexes()
> db.system.indexes.find()
> db.people.dropIndexes() / db.people.dropIndex({age:1})
> db.people.reIndex()   # rebuild all indexes for a collection

Index entries have a limitation on their maximum size (the sum of the values), currently approximately 800 bytes.
This limitation will eventually be removed ...
  
Shell execution
> runProgram("uname", "-a");
> runProgram("ls", "-l");
===============================================================================


The samson module that uses all this ...

--------- modules/txt_md/export_to_mongo.h ----------------------------------------
#include "mongo/client/dbclient.h"

DBClientConnection*  mdbConnection;

void init(samson::KVWriter* writer)
{
    mongo_ip           = environment->get("mongo.ip",    "no-mongo-ip");
    mongo_db           = environment->get("mongo.db",    "no-mongo-db");
    mongo_collection   = environment->get("mongo.collection", "no-mongo-collection");

    mongo_db_path = mongo_db + "." + mongo_collection;

    mdbConnection = new DBClientConnection();
    mdbConnection->connect(mongo_ip);
}

void run(samson::KVSetStruct* inputs, samson::KVWriter* writer)
{
    // Check mongo_ip, mongo_db and mongo_collection ...

    for (size_t i = 0 ; i < inputs[0].num_kvs ; i++)
    {
        samson::system::String  key;
        samson::system::UInt    value;

        key.parse(inputs[0].kvs[i]->key);
        value.parse(inputs[0].kvs[i]->value);

        bo = BSON("K" << key.value << "V" << (long long) value.value);

        mdbConnection->insert(mongo_db_path, bo);
    }

    mdbConnection->ensureIndex(mongo_collection, fromjson("{key:0}"));
}

-------------------------------------------------------------------------------

--------- modules/txt_md/import_from_mongo.h ----------------------------------

void run(samson::KVWriter* writer)
{
    // Check mongo_ip, mongo_db and mongo_collection ...

    BSONObj                 bo;
    samson::system::String  K;
    samson::system::UInt    V;

    auto_ptr<DBClientCursor> cursor = mdbConnection->query(mongo_db_path);

    while (cursor->more())
    {
        bo       = cursor->next();
        K.value  = bo.getStringField("K");
        V.value  = bo.getIntField("V");

        writer->emit(0, &K, &V);
    }
}

--------- modules/txt_md/mongo_bulk_load.h ----------------------------------

void run(samson::KVSetStruct* inputs, samson::KVWriter* writer)
{
    // Check mongo_ip, mongo_db and mongo_collection ...

    std::vector<mongo::BSONObj> bulk_data;
    for (size_t i = 0 ; i < inputs[0].num_kvs ; i++)
    {
        samson::system::UInt      key;
        samson::txt_md::BulkData  value;

        key.parse(inputs[0].kvs[i]->key);
        value.parse(inputs[0].kvs[i]->value);

        mongo::BSONObj record = BSON(  "I" << (long long int) key.value 
                                    << "P" << (long long int) value.position.value
                                    << "T" << (long long int) value.timestamp.value);
        bulk_data.push_back(record);

		// Check size of record to see if it's time to call 'insert'
    }

    mdbConnection->insert(mongo_db_path, bulk_data);
    bulk_data.clear();
}

===============================================================================

Start a Mongodb (in torkel, for example):
kzangeli@torkel:~> /opt/mongodb/mongodb-linux-x86_64-2.0.0/bin/mongod

Start samson (in torkel):
kzangeli@torkel:~> samsonSpawner -local -fg

Start delilah (in torkel):
kzangeli@torkel:~> delilah
Delilah> add mongoQueryOutputQueue system.UInt cdr.mobCdr
Delilah> remove_all
Delilah> txt.generate_words words -create
Delilah> txt.export_words words words.txt -create
Delilah> download words.txt words.txt 
Delilah> txt_md.export_to_mongo words -mongo.ip localhost -mongo.db xtestdb -mongo.collection xtestcoll

Now enter a Mongo shell (in torkel):
kzangeli@torkel:~> mongo
> use xtestdb
> show collections
> db.xtestcoll.find()

Back to Delilah:
Delilah> remove_all
Delilah> txt_md.import_from_mongo words -create -mongo.ip localhost -mongo.db xtestdb -mongo.collection xtestcoll
Delilah> txt.export_words words words.txt -create
Delilah> download words.txt words2.txt 


===============================================================================
Now, compare a search with and without indexes ...

> db.xtestcoll.find({K:"cgbigg"})
  => Pretty slow !

> db.xtestcoll.ensureIndex({K:1})
> db.xtestcoll.find({K:"cgbigg"})
  => Takes a lot less time ...


===============================================================================

REMOVING STUFF IN Mongo ...
> db.people.remove()                  # remove all objects from the PEOPLE collection
> db.people.drop()                    # drop the entire PEOPLE collection
> db.people.remove({name : "sara"})   # drop the objects matching the query
> db.dropDatabase()                   # drop the current database


===============================================================================

SHARDS
o Start the individual shards (mongod), config servers (mongod --configsvr), and routers (mongos) processes
  mongos: Specify the --configdb parameter to indicate location of the config database
  start the mongo shell, connecting it to the mongos process (giving its port)
  In samson06 (8 cores) we will start 8 shards, one config server and one mongoS

    % sudo mkdir /data/configdb
    % sudo chown samson:samson /data/configdb
    % sudo mkdir /data/db /data/db2 /data/db3 /data/db4 /data/db5 /data/db6 /data/db7 /data/db8
    % sudo chown samson:samson /data/db2 /data/db3 /data/db4 /data/db5 /data/db6 /data/db7 /data/db8

    % numactl --interleave=all mongod --nojournal --shardsvr --port 2001 --dbpath /data/db
    % numactl --interleave=all mongod --nojournal --shardsvr --port 2002 --dbpath /data/db2
    % numactl --interleave=all mongod --nojournal --shardsvr --port 2003 --dbpath /data/db3
    % numactl --interleave=all mongod --nojournal --shardsvr --port 2004 --dbpath /data/db4
    % numactl --interleave=all mongod --nojournal --shardsvr --port 2005 --dbpath /data/db5
    % numactl --interleave=all mongod --nojournal --shardsvr --port 2006 --dbpath /data/db6
    % numactl --interleave=all mongod --nojournal --shardsvr --port 2007 --dbpath /data/db7
    % numactl --interleave=all mongod --nojournal --shardsvr --port 2008 --dbpath /data/db8
    
    % numactl --interleave=all mongod --configsvr --port 2101 

	# For various config servers: --configdb samson01:2101,samson02:2101,samson03:2101
    % mongos --configdb localhost:2101 # Let mongos use the default port 27017 ...
	
    % ./mongo --port 27017             # with the '--port' option you choose the port of mongod/mongos
    mongo> use admin

o Add shards in 'mongo' (in 'admin' db):
  mongo> use admin
  mongo> db.runCommand( { addshard : "localhost:2001" } );
  mongo> db.runCommand( { addshard : "localhost:2002" } );
  mongo> db.runCommand( { addshard : "localhost:2003" } );
  mongo> db.runCommand( { addshard : "localhost:2004" } );
  mongo> db.runCommand( { addshard : "localhost:2005" } );
  mongo> db.runCommand( { addshard : "localhost:2006" } );
  mongo> db.runCommand( { addshard : "localhost:2007" } );
  mongo> db.runCommand( { addshard : "localhost:2008" } );
  {"ok" : 1 , "added" : ...}


===============================================================================
db.runCommand( { addshard : "samson05:2001" } )
db.runCommand( { addshard : "samson06:2001" } )
db.runCommand( { addshard : "samson07:2001" } )
db.runCommand( { addshard : "samson08:2001" } )
use admin
db.runCommand( { enablesharding : "test" } )
use test
db.lloc01.ensureIndex({"I": 1, "T": 1})
db.ploc01.ensureIndex({"I": 1, "T": 1})
use admin
db.runCommand( { shardcollection : "test.lloc01", key : { "I": 1, "T" : 1 } } )
db.runCommand( { shardcollection : "test.ploc01", key : { "I": 1, "T" : 1 } } )
use test
show collections
db.lloc01.find().count()
db.ploc01.find().count()

===============================================================================

o Define shard key
  mongo> use admin
  mongo> db.runCommand( { enablesharding : "test" } );   # if not existing, 'use' it first - then go back to 'admin' db
  mongo> use test
  mongo> db.test.ensureIndex({"I": 1, "T": 1});
  mongo> use admin
  mongo> db.runCommand( { shardcollection : "test.lloc01", key : { "I": 1, "T" : 1 } } )

o Start samson and delilah in Torkel
  torkel> samsonSpawner -local -fg

 Run a test just to create the database and a collection:
  % cd ~/passiveLocationTest/
  delilah> remove_all
  delilah> add cdrIn -txt
  delilah> upload XMLs cdrIn
  delilah> mob2.parse_xml_ARCANUM cdrIn cdrOut null1 null2 null3 null4 -c
  delilah> passive_location.mongo_location_export cdrOut mongo.ip=samson06:27017 mongo.db=test mongo.lastloc_db=test mongo.collection=lloc01 mongo.lastloc_collection=ploc01

========================================================================================================================

Examining status of sharding from 'mongo':

  o Identifying a Shard Cluster
    Are we connected to mongos or mongod?

    mongo> db.runCommand({ isdbgrid : 1});

    If mongod, this command returns a "no such cmd" message.
    If mongos: { "ismaster": true, "msg": "isdbgrid", "maxBsonObjectSize": XXX, "ok": 1 }
           or: { "isdbgrid" : 1, "hostname" : "samson07", "ok" : 1 }

  o List Existing Shards
    mongo> use admin
    mongo> db.runCommand({ listShards : 1});

  o List Which Databases are Sharded - you need to have one first ...
    mongo> config = db.getSisterDB("config")
    mongo> config.system.namespaces.find()

  o View Sharding Details
    mongo> use admin
    mongo> db.printShardingStatus();

  o Removing a shard
    mongo> db.runCommand( { removeshard : "shard0001" } );


*******************************************************************************

Samson and MongoDB integration document - for Passive Location
================================================================================================================================

1. Introduction - why external storage
------------------------------------------------------------------------------------------------------------------------------
Samson is a batch and streaming platform for BigData, not an external storage solution, like Cassandra or any other NoSQL database.
Though, in the intent to make Samson a stand-alone procuct, there is a need for a means to store data, both
for incoming data and the results obtained, and also a mechanism to query this data.
 
The Samson team has researched a number of different external data storage solutions and also the idea to
implement the external storage from scratch, perfectly fitting Samson needs.
This latter plan was soon rejected because of the huge effort it would take to implement such a project.

Finally the decision has fallen on using MongoDB, as it is a distributed and fail-safe solution.
The fact that recent clients push for MongoDB has influenced in this decision.


2. MongoDB
------------------------------------------------------------------------------------------------------------------------------

      Introduction to mongo ...



2.1 Executables
------------------------------------------------------------------------------------------------------------------------------
MongoDB comes with lots of components and utilities:

  o mongod     The database process.
  o mongos     Sharding controller.
  o mongo      The database shell (uses interactive javascript).

  Using these three components, any MongoDB configuration can be setup and tested. 



2.2   Simple Mongo Setup
------------------------------------------------------------------------------------------------------------------------------
The first tests made used one single Mongo server. This is a very simple setup and all that is needed is to execute a
'mongod' process in the machine supposed to serve the data base. 

  To run a single server database:

    $ mkdir /data/db  # create the directory where the data is to be stored - configurable, of course ...
    $ ./mongod

    $ # The mongo javascript shell connects to localhost and test database by default:
    $ ./mongo 
    > help



2.3   Redundancy and Failover
------------------------------------------------------------------------------------------------------------------------------
In order to protect the MongoDB data against node failure, any number of secondary 'mongod' processes can be used,
standing by to take over the execution in case the primary 'mongod' process fails.
In MongoDB this concept is referred to as 'Replica Sets'.

Each of these secondary processes should of course be running in separate computers, and this naturally adds to the
total cost of the platform.
Depending on how important the data is, it is normally enough to have a single backup node, accompanied by an arbiter,
which doesn't need a separate machine. Varios arbiters can run on a single server.

Arbiters are nodes in a replica set that only participate in elections: they don't have a copy of the data and will never
become the primary node (or even a readable secondary). They are mainly useful for breaking ties during elections
(e.g. if a set only has two members).



2.4 Indexes
------------------------------------------------------------------------------------------------------------------------------
Short explanation on MongoDB indexes



2.5 Sharding
------------------------------------------------------------------------------------------------------------------------------
To increase MongoDB performance, a collection can be distributed among varios nodes (computers).
This concept has received the name 'sharding' and has the advantage that inserts and queries are
distributed over a number of nodes and are thus faster.



2.6   Sharded Mongo Setup Example
------------------------------------------------------------------------------------------------------------------------------



3.    Samson
------------------------------------------------------------------------------------------------------------------------------
      Introduction to Samson



3.1   Samson modules to extend functionality
------------------------------------------------------------------------------------------------------------------------------

4.    Passive Location Pilot
------------------------------------------------------------------------------------------------------------------------------
      Introduction ...



4.1   Mongo setup - what to store in the collections
------------------------------------------------------------------------------------------------------------------------------



4.2   Samson setup
------------------------------------------------------------------------------------------------------------------------------



4.3   Arcanum Input for Samson
------------------------------------------------------------------------------------------------------------------------------
The data that is injected in samson from ARCANUM is in XML form and needs to be parsed in order to extact the
relevant data, which for one single record consists of the following fields:
- EMEI
- Cell tower ID
- Timestamp
- etc.




4.4   Expectations
------------------------------------------------------------------------------------------------------------------------------
The maximum desired accessability for the passive location pilot with O2 UK is for the samson platform to be able to store
40k location input events per second in MongoDB and at the same time being able to serve 100 queries per second.
Two separate data base tables (in MongoDB the corresponding concept is 'collections') will be used for this purpose.

The first of these MongoDB collections (from here on referred to as the 'Input Collection') will hold the raw input
(but stripped down to only contain the relevant data, of course), while the second will hold only one record (the corresponding
MongoDB concept is 'document') per user/cell phone number, describing the last known location of this subscriber.



5.    Pilot Implementation Details
------------------------------------------------------------------------------------------------------------------------------



5.1   SAMSON implementation details
------------------------------------------------------------------------------------------------------------------------------
Samson is a modular platform able to execute third-party modules as a part of the platform.
These modules are compiled as separate shared libraries and Samson links to them in run-time.
This mechanism has been used to implement Samson modules that connect Samson with MongoDB in a seemless manner.

The idea with these samson-mongodb modules is to not include any MongoDB administration into Samson,
but merely use the databases and collections that a Samson-user inputs as parameters to the platform.
All mongodb administration and decision, such as use shards, will be done outside of Samson, using standard MongoDB tools.

A special Samson module to parse the input XML data from Arcanum is developed ...



5.1.1 Module 1: XML parser for input
------------------------------------------------------------------------------------------------------------------------------



5.1.2 Module 2: Details on mongo module? parameters, usage ... ?
------------------------------------------------------------------------------------------------------------------------------



5.2   MongoDB setup details
------------------------------------------------------------------------------------------------------------------------------



5.2.1 Sharding Key selection
------------------------------------------------------------------------------------------------------------------------------



5.2.2 Collection Splitting
------------------------------------------------------------------------------------------------------------------------------
Inserting documents in a MongoDB collection takes longer the bigger the collection is, so the idea is to split the
Input Collection in time frames. Later on, the decision on the size of these time frames will be taken.
What seems to be true is that if a collection grows to double its size, the insertion speed goes down with 50%.
So, comparing to saving all data in one single collection, the inserts will be six times faster if we split the
input data in one separate collection per month (or 365/2 times faster if the split is done per day ...).
This 'collection splitting' makes querying a bit more difficult as a user will have to work against a number
of collections instead of just against a single collection, but 

QUESTION: Will any queries be done on 'Input Collection'?
          If not, why index it?  Isn't it true that an indexed collection get worse insertion time?



6.    Test Results
------------------------------------------------------------------------------------------------------------------------------
Two different setups has been used to extract sufficient test results for this 'proof-of-concept':

1. One simple setup with only one MongoDB node, used with an external tool to upload data to mongodb
   and to at the same time querying that same collection. The idea behind this simple test is to verify how
   much load a single (non-sharded) mongodb node is able to cope with, at the same time as querying is performed 
   against that collection. This may not be 100% relevant as main part of the queries will be against the
   database/collection that contains only the 'last known location' of each of the users in the pilot.
   However, the test gives us an initial idea on how many shards we will need to cope with 40000 KVs/s and
   100 qps.

2. The second setup is as close to the real pilot setup as possible - four samson nodes and four MongoDB nodes and
   all data uploads are executed using the samson mongo module implemented for the passive location pilot.
   The MongoDB secondary nodes aren't even started, and we must bear in mind that the replication of the data
   will slow down the entire system.



6.1. A first insert and lookup stress-test in a single mongo node
------------------------------------------------------------------------------------------------------------------------------
The throughput tests were performed using the simple setpup, with a single 'mongod' process.
We're assuming that each shard will add more or less this same throughput.

The table below describes the number of queries per second at different insertion rates and
starting with different sizes of the collection that is at the same time both inserted into and queried.

For the first row of the table, the collection was first injected with 207 million key-value pairs and
without any simultaneous inserts, a throughput of 56 queries per second was achieved.
When injecting 40 new key-values per second, the responsiveness surprisingly went up to 58 queries per second.
However, 40 queries per second is not very much and this strange result is probably just fluctuations ...


     KVs    |  0    40    80    200   400   800  4000   8000  16000
------------------------------------------------------------------
 207000000  | 56  | 58  | 64  |  -  |  -  |  -  |  -  |  X  |  X  |
------------------------------------------------------------------
 103500000  |2790 |2760 |2750 |  -  |  -  |  -  |  -  |  X  |  X  |
------------------------------------------------------------------
  69000000  |2900 |2900 |3000 |2800 |1900 |1450 |1050 |  X  |  X  |
------------------------------------------------------------------
  51750000  |3200 |3000 |2900 |2900 |2700 |2700 |1500 |  X  |  X  |
------------------------------------------------------------------
  41400000  |3000 |3000 |2900 |3000 |3150 |3150 |2950 | 2700| 1700|
------------------------------------------------------------------
  20700000  |3000 |3050 |3150 |3150 |2950 |3000 |2800 |2500 |2250 |
------------------------------------------------------------------
  10350000  |3000 |3050 |3050 |3050 |3100 |3150 |3000 |2850 |2100 |
------------------------------------------------------------------
   2070000  |3150 |3150 |3180 |3100 |3050 |3125 |2900 |2750 |2400 |
------------------------------------------------------------------

The initial value of 207 million key values is taken from the total number of
key-values that the database would contain after storing 40 events per second,
eight hours a day, during six months:

40 eps * 3600 secs * 8 hours * 30 days * 6 months  == 207 million key-value pairs

We have found a maximum of a little more than 3000 queries per second, and this depends on the
round-trip time of a TCP/IP message. In order to do a query, a message is sent to the 'mongod' process 
and the response is waited for. 3000 queries per second implies that one query takes around 0.3 millisceonds
and that's about as good as it gets, with Gigabit Ethernet, which is what was used during these tests.

KZ NOTE:
Really, this test was done querying the 'big collection' when in reality the queries will go to the
'last known location' collection, so perhaps the test isn't that interesting ...



6.2. Test results samson/mongo integration
------------------------------------------------------------------------------------------------------------------------------
This test setup includes four nodes running a samson cluster, and another four nodes running a mongodb cluster.
Using the samson utility 'passiveLocationPush', pushing 10000 key-values to the samson platform, working in streaming mode,
the (fake) records are inserted and the samson platform is without any load worth mentioning.

Testing with a 'whopping' 80000 key-values per second for inserts, the samson load goes up to 16 used cores (ALL cores) but
still, all data is perfectly inserted and no records are lost.

Now, executing an external tool to query the database, we get results of X queries per second.

    40 KVs => X qps
  1000 KVs => X qps
  4000 KVs => X qps
 10000 KVs => X qps
 20000 KVs => X qps
 40000 KVs => X qps
 80000 KVs => X qps
160000 KVs => X qps



7. Conclusions
------------------------------------------------------------------------------------------------------------------------------
A setup consisting of four samson nodes and eight mongo nodes (half of the mongo nodes running as secondary mongo servers for redundancy)
the entire platform is easily able to store 40000 KVs/s and simultaneously respond to 100 queries per second (on the last know location 
database). When querying the history database collection, we reach XXX qps.



8. Remarks
------------------------------------------------------------------------------------------------------------------------------
o Journaling will not be used
   Journaling cannot be used (around ten times slower!).
   If redundancy is needed, another node should be used: REPLICA SETS (primary/secondary)
   For this, an extra machine is needed per shard ...
   But also, this gives complete redundancy and availability.
   Tests should be performed putting the journal on a separate hard drive.
   According to the nice people at 10gen, this slowdown we've seen is far from normal ...

o Sharding will be used
   We will not have enough with just ONE machine for Mongo.
   Sharding is supposedly buggy (at least in mongo release 1.8) but ...
   I have the list from Theo about what kind of bugs he encountered with sharding (with Mongo 1.8)

o Collections will be split month by month or in even smaller chunks.
   The smaller the collections are, the faster searches we will have ...

o Found a roof of around 3000 queries per second.
  After consulting with Mattias of 10gen, I'm sure this is simply the network latency.
  Tests should be done querying from localhost to make sure ...

o Avoid having collections bigger than 50M records (7.5 Gigabytes on disk)

o Indexing makes the insert time almost 100% slower.
   But, searches are impossible without indexes, so indexing must be used
   If the index is kept on a separate (and faster) disk, there is lots of throughput to gain

========================================================================================================================


========================================================================================================================
BIG TEST:

samson01 - samson04 : Samson platform (Andreu starts it)
samson05 - samson07 : MongoDB shard for 'History'
samson08 - samson09 : MongoDB shard for 'Last Known Location'

samson05-samson09:
  sudo rm -rf /data/db
  sudo mkdir /data/db
  sudo chown samson:samson /data/db
  numactl --interleave=all mongod --nojournal --shardsvr --port 2001

samson05+samson08
  sudo rm -rf /data/configdb
  sudo mkdir /data/configdb
  sudo chown samson:samson /data/configdb
  numactl --interleave=all mongod --configsvr --port 2101

  mongos --configdb samson05:2101
  mongos --configdb samson08:2101

  mongo --port 27017

samson05 (mongo):
  use admin
  db.runCommand( { addshard : "samson05:2001" } );
  db.runCommand( { addshard : "samson06:2001" } );
  db.runCommand( { addshard : "samson07:2001" } );
  db.runCommand( { enablesharding : "PassiveLocation" } )
  use PassiveLocation
  db.History.ensureIndex({"I": 1, "T": 1})
  use admin
  db.runCommand( { shardcollection : "PassiveLocation.History", key : { "I": 1, "T" : 1 } } )
  use PassiveLocation
  db.History.find().count()

samson08 (mongo):
  use admin
  db.runCommand( { addshard : "samson08:2001" } );
  db.runCommand( { addshard : "samson09:2001" } );
  db.runCommand( { enablesharding : "PassiveLocation" } )
  use PassiveLocation
  db.LastKnownLocation.ensureIndex({"_id": 1})
  use admin
  db.runCommand( { shardcollection : "PassiveLocation.LastKnownLocation", key : { "_id": 1 } } )
  use PassiveLocation
  db.LastKnownLocation.find().count()

delilah (in torkel) to start streaming pipeline
  cd modules/passive_location
  delilah -controller samson01 -f stream_preparation.txt
  delilah -controller samson01 -f stream_pl_history.txt
  delilah -controller samson01 -f stream_pl_last_known_location.txt

passiveLocationPush (in some machine)
  passiveLocationGenerator 10000 | samsonPush pl.in_xml_cdrs -v -buffer_size 10000 -timeout 1


queries to PassiveLocation.LastKnownLocation:
  ./BUILD_DEBUG/testing/test_mongo/test_mongo -server samson08:27017 -db PassiveLocation -coll LastKnownLocation -query -queries 5000

------------------------------------------------------------------------------------------------------------------------

TEST RESULTS:
1. 'LastKnownLocation' filled with 9,000,000 users (4 samsons, 3 mongo.History and 2 mongo.LastKnownLocation)

       0 KVs/s:  3800 qps
	 400 KVs/s:  3500 qps
    1000 KVs/s:  2900 qps
    3000 KVs/s:  1100 qps
	3500 KVs/s:   500 qps
    4000 KVs/s:   100 qps

It seems we get to a total of around 4000 operations per second, between inserts and queries.
It might very well be the network that is the limiting factor.

Pure insert:
  2 shards:   22000 inserts/sec (with index)
  1 'shard':  50000  inserts/sec (with index, without sharding, bulksize: 5000)
  1 'shard':  73000  inserts/sec (with index, without sharding, bulksize: 50000)
  1 'shard':  100000 inserts/sec (with index, without sharding, bulksize: 5000) - TOO MUCH !!!
  1 'shard':  100000 inserts/sec (w/o index) - no probs - building index takes 40 secs (9.000.000 users)

  We've seen that the inserts are MUCH faster if we let mongodb select the '_id' field ...
  
2. Same test, but a single mongo 'shard' (9,000,000 users)

       0 KVs/s:  3800 qps   (in local: 20000)
	 400 KVs/s:  3400 qps
    1000 KVs/s:  2900 qps   (in local: 11772)
    3000 KVs/s:  1100 qps
	3500 KVs/s:   740 qps   (in local: 2800)
    4000 KVs/s:   250 qps   (in local:  100)

3. Same test but now samson runs in same machine as mongo - compare same/different


2. Same test, but in a cluster of three nodes, see if SUMMA goes up to 6000



===============================================================================

Testing db.LastKnownLocation.update ...

# Empty collection
db.LastKnownLocation.drop()

# Insert a document, _id '1', timestamp '1'
db.LastKnownLocation.insert( { _id:1, T:1, C:1, X:1, Y:1 });
db.LastKnownLocation.find()

# Update that only document, changing the timestamp to 2
db.LastKnownLocation.update( { _id:1 }, { _id:1, T:2, C:1, X:1, Y:1 }, true  )
db.LastKnownLocation.find()

# Update that only document, changing the timestamp to 3, but only if the old timestamp is 'less than 3' (which it is!)
db.LastKnownLocation.update( { _id:1, T : { $lt: 3 }  }, { _id:1, T:3, C:1, X:1, Y:1 }, true  )
db.LastKnownLocation.find()

# Update that only document, changing the timestamp to 2, but only if the old timestamp is 'less than 2' (which it is NOT!)
db.LastKnownLocation.update( { $and: [ { _id:1 },  { T : { $lt: 2 }  } ] }, { _id:1, T:2, C:1, X:1, Y:1 }, true  )
db.LastKnownLocation.find()
# The result is an error, no document found with this condition, but as a document with _id 1 exists, the INSERT fails. All Good!

# Add another document, _id: "2", timestamp: "2"
db.LastKnownLocation.update( { _id:2 }, { _id:2, T:2, C:1, X:1, Y:1 }, true  )
db.LastKnownLocation.find()
